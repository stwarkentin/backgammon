{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e7ff66-7905-4f12-9330-2189d8dca11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 11:44:14.787958: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-12 11:44:14.791367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-12 11:44:14.791379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from dice import roll\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.spaces import Dict, Box, Discrete\n",
    "import numpy as np\n",
    "from random import random, choice\n",
    "from copy import copy, deepcopy\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a23a20-5307-42d1-a02d-e86759f41d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgammonEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        # defining some ranges for our observation space\n",
    "        low = np.zeros((96,)) # 4 per each of the 24 positions on the board\n",
    "        high = np.ones((96,)) # the first three of the four values encoding the number of checkers at a given position are either 0 or 1...\n",
    "\n",
    "        for i in range(3, 97, 4): # but every fourth value can go as high as (15-3)/2 = 6...\n",
    "            high[i] = 6.0\n",
    "\n",
    "        # to make them more readable to humans, our observations are dictionaries\n",
    "        # but we can't feed dictionaries to our ANN, so we are going to have to use gym's 'FlattenObservation' wrapper to flatten the dictionary into a single array later\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                # player 'White'\n",
    "                'W': Dict(\n",
    "                    {\n",
    "                        'board': Box(low=low, high=high,dtype=np.float32), # the board, consisting of the bar and 24 'points'\n",
    "                        'barmen': Box(low=0.0, high=7.5,shape=(1,),dtype=np.float32), # and the very first value, which encodes the number of checkers on the bar, can go as high as 15/2 = 7.5\n",
    "                        'menoff': Box(low=0.0,high=1.0,shape=(1,),dtype=np.float32), # number of checkers removed from the board as a fraction of the total number of checkers i.e. n/15\n",
    "                        'turn': Discrete(2) # '1' if it is this player's turn, '0' if not\n",
    "                    }\n",
    "                ),\n",
    "                # player 'Black'\n",
    "                'B': Dict(\n",
    "                    {\n",
    "                        'board': Box(low=low, high=high,dtype=np.float32),\n",
    "                        'barmen': Box(low=0.0,high=7.5,shape=(1,),dtype=np.float32), \n",
    "                        'menoff': Box(low=0.0,high=1.0,shape=(1,),dtype=np.float32),\n",
    "                        'turn': Discrete(2)\n",
    "                    }\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # which actions can be taken at any given time step depend on the state of the board and the roll of the dice. hence we choose to not attempt to define the action space\n",
    "        self.action_space = None\n",
    "        \n",
    "        # create an empty board:\n",
    "        # to allow for indexing of board positions, the environment's state uses an array of arrays to store the values of the bar and each point\n",
    "        # later we simply flatten this array to create observations\n",
    "\n",
    "        self.state = {\n",
    "            'W': {\n",
    "                'board': np.zeros((24, 4)),\n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': 0\n",
    "            },\n",
    "            'B': {\n",
    "                'board': np.zeros((24, 4)),\n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # the previously mentioned truncated unary encoding:\n",
    "        self.encoding = {\n",
    "            0: np.array([0.,0.,0.,0.]),\n",
    "            1: np.array([1.,0.,0.,0.]),\n",
    "            2: np.array([1.,1.,0.,0.]),\n",
    "            3: np.array([1.,1.,1.,0.]),\n",
    "            4: np.array([1.,1.,1.,0.5]),\n",
    "            5: np.array([1.,1.,1.,1.]),\n",
    "            6: np.array([1.,1.,1.,1.5]),\n",
    "            7: np.array([1.,1.,1.,2.0]),\n",
    "            8: np.array([1.,1.,1.,2.5]),\n",
    "            9: np.array([1.,1.,1.,3.0]),\n",
    "            10: np.array([1.,1.,1.,3.5]),\n",
    "            11: np.array([1.,1.,1.,4.0]),\n",
    "            12: np.array([1.,1.,1.,4.5]),\n",
    "            13: np.array([1.,1.,1.,5.0]),\n",
    "            14: np.array([1.,1.,1.,5.5]),\n",
    "            15: np.array([1.,1.,1.,6.0])\n",
    "        }\n",
    "\n",
    "        # define the game's starting position:\n",
    "  \n",
    "        # create an empty board\n",
    "        self.starting_pos = np.zeros((24, 4))\n",
    "\n",
    "        # place the correct number of checkers in the correct positions\n",
    "        self.starting_pos[0] = copy(self.encoding[2])\n",
    "        self.starting_pos[11] = copy(self.encoding[5])\n",
    "        self.starting_pos[16] = copy(self.encoding[3])\n",
    "        self.starting_pos[18] = copy(self.encoding[5])\n",
    "\n",
    "    def _flatten_obs(self, obs):\n",
    "        w_board = obs['W']['board']\n",
    "        w_board = w_board.flatten()\n",
    "        b_board = obs['B']['board']\n",
    "        b_board = b_board.flatten()\n",
    "\n",
    "        observation = []\n",
    "        observation = np.append(observation,w_board)\n",
    "        observation = np.append(observation,obs['W']['barmen'])\n",
    "        observation = np.append(observation,obs['W']['menoff'])\n",
    "        observation = np.append(observation,obs['W']['turn'])\n",
    "        observation = np.append(observation,b_board)\n",
    "        observation = np.append(observation,obs['B']['barmen'])\n",
    "        observation = np.append(observation,obs['B']['menoff'])\n",
    "        observation = np.append(observation,obs['B']['turn'])\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_info(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self): \n",
    "\n",
    "        # a 'coin flip' to determine which side goes first\n",
    "        coin = int(random()>0.5)\n",
    "\n",
    "        # reset the board to the game's starting position and assign a turn order based on the coin flip\n",
    "        self.state = {\n",
    "            'W': {\n",
    "                'board':copy(self.starting_pos), \n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': coin\n",
    "            },\n",
    "            'B': {\n",
    "                'board':copy(self.starting_pos),\n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': 1-coin\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return copy(self.state)\n",
    "\n",
    "    def step(self, prediction, action):\n",
    "\n",
    "        if prediction:\n",
    "            state = deepcopy(self.state)\n",
    "        else:\n",
    "            state = self.state\n",
    "\n",
    "        # who's turn is it?\n",
    "        if self.state['W']['turn'] == 1:\n",
    "            player = 'W'\n",
    "            opponent = 'B'\n",
    "        else:\n",
    "            player = 'B'\n",
    "            opponent = 'W'\n",
    "\n",
    "        for move in action:\n",
    "            # 'LIFTING' A CHECKER\n",
    "\n",
    "            old_pos, new_pos = move\n",
    "            \n",
    "            # are we moving a piece off the bar?\n",
    "            if old_pos == -1:\n",
    "                # remove a checker from the bar\n",
    "                state[player]['barmen'] -= 0.5\n",
    "\n",
    "            else:\n",
    "                # get the current number of checkers at the position from which we need to remove a checker\n",
    "                encoded_checkers = state[player]['board'][old_pos]\n",
    "                # decode\n",
    "                for key, value in self.encoding.items():\n",
    "                    if np.array_equal(encoded_checkers,value):\n",
    "                        n_checkers = key\n",
    "                # subtract a checker\n",
    "                state[player]['board'][old_pos] = copy(self.encoding[n_checkers-1])\n",
    "\n",
    "            # 'PLACING DOWN' A CHECKER\n",
    "\n",
    "            # are we bearing off?\n",
    "            if new_pos == 24:\n",
    "                state[player]['menoff'] += 1/15\n",
    "\n",
    "            else:\n",
    "                # get the current number of checkers at the position to which we need to add a checker\n",
    "                encoded_checkers = state[player]['board'][new_pos]\n",
    "                # decode\n",
    "                for key, value in self.encoding.items():\n",
    "                    if np.array_equal(encoded_checkers,value):\n",
    "                        n_checkers = key\n",
    "                # add a checker\n",
    "                state[player]['board'][new_pos] = copy(self.encoding[n_checkers+1])\n",
    "\n",
    "                # check for blots\n",
    "                mirror_pos = new_pos+23-2*new_pos\n",
    "                if not np.array_equal(state[opponent]['board'][mirror_pos],[0,0,0,0]):\n",
    "                    # if there is a blot, move the opponent's piece to the bar\n",
    "                    state[opponent]['board'][mirror_pos] = [0,0,0,0]\n",
    "                    state[opponent]['barmen'] += 0.5\n",
    "\n",
    "        # update the turn order\n",
    "        state['W']['turn'] = 1 - state['W']['turn']\n",
    "        state['B']['turn'] = 1 - state['B']['turn']\n",
    "\n",
    "        # if this is only a 'simulated' step, return the new state here\n",
    "        if prediction:\n",
    "            return self._flatten_obs(state)\n",
    "\n",
    "        # reward is zero unless one of four conditions is met:\n",
    "        reward = 0\n",
    "\n",
    "        # 1) White wins, Black is gammoned\n",
    "        if self.state['W']['menoff'] > 0.9 and self.state['B']['menoff'] == 0:\n",
    "            reward = 2\n",
    "        # 2) White wins\n",
    "        elif self.state['W']['menoff'] > 0.9:\n",
    "            reward = 1\n",
    "        # 3) Black wins, White is gammoned\n",
    "        elif self.state['B']['menoff'] > 0.9 and self.state['W']['menoff'] == 0: \n",
    "            reward = -2\n",
    "        # 4) Black wins\n",
    "        elif self.state['B']['menoff'] > 0.9:\n",
    "            reward = -1\n",
    "\n",
    "        # if one of the four conditions is met, the game is finished and the episode ends\n",
    "        done = reward != 0\n",
    "\n",
    "        return copy(self.state), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2747007-0ebe-459b-a187-6cd90119a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, network):\n",
    "        self.env = env\n",
    "        #self.gamma = gamma\n",
    "        self.network = network        \n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        \n",
    "        dice = roll()\n",
    "        \n",
    "        legal_actions = []\n",
    "        size = len(dice)\n",
    "        action = []\n",
    "\n",
    "        # check whose turn it is\n",
    "        if obs['W']['turn'] == 1:\n",
    "            player = 'W'\n",
    "            opponent = 'B'\n",
    "        else:\n",
    "            player = 'B'\n",
    "            opponent = 'W'\n",
    "            \n",
    "        player_obs = deepcopy(obs[player])\n",
    "        opponent_obs = obs[opponent]\n",
    "        \n",
    "        # make board more easily readable\n",
    "        new_board = []\n",
    "        for pos in player_obs['board']:\n",
    "            new_board.append(pos[0] + pos[1] + pos[2] + pos[3] * 2)\n",
    "        player_obs['board'] = new_board\n",
    "        \n",
    "        # !!! just for testing purposes !!!\n",
    "        # if not length == size and not len(action) == 0:\n",
    "        #test_opponent = deepcopy(opponent_obs)\n",
    "        #new_op_board = []\n",
    "        #for idx, pos in enumerate(test_opponent['board']):\n",
    "        #    new_op_board.append(pos[0] + pos[1] + pos[2] + pos[3] * 2)\n",
    "        #test_opponent['board'] = new_op_board\n",
    "        #print(\"Dice: \", dice)\n",
    "        #print(\"Player:\\t\\t\", player, player_obs['board'])\n",
    "        #print(\"Opponent:\\t\", opponent, test_opponent['board'])\n",
    "        \n",
    "        # recursive function to search \"action-tree\"\n",
    "        def find_board_actions(action, dice, player_obs):\n",
    "            \n",
    "            # if we have iterated through all dice the action is appended and we return\n",
    "            if len(dice) == 0:\n",
    "                legal_actions.append(action)\n",
    "                return\n",
    "\n",
    "            # in case there are chips in the bar they have to be removed before any other actions can be taken\n",
    "            if player_obs['barmen'] > 0:\n",
    "                for i, die in enumerate(dice):\n",
    "                    if opponent_obs['board'][die + 23 - 2 * die][1] == 0:\n",
    "                        new_player_obs = deepcopy(player_obs)\n",
    "                        new_player_obs['board'][die] += 1\n",
    "                        new_player_obs['barmen'] -= 0.5\n",
    "                        find_board_actions(action.copy() + [(-1, die)], dice[1:], new_player_obs)\n",
    "                \n",
    "                        \n",
    "            else:\n",
    "                # is it legal to move off the board?\n",
    "                moveoff = True\n",
    "                for idx in range(18):\n",
    "                    if player_obs['board'][idx] > 1:\n",
    "                        moveoff = False\n",
    "                        break\n",
    "                        \n",
    "                # iterate through all poitions and check if we can move to position + current dice\n",
    "                for idx, pos in enumerate(player_obs['board']):\n",
    "                    if pos > 0 and (idx + dice[0]) > 23:\n",
    "                        if moveoff:\n",
    "                            new_player_obs = deepcopy(player_obs)\n",
    "                            new_player_obs['board'][idx] -= 1\n",
    "                            # recursively call function\n",
    "                            find_board_actions(action.copy() + [(idx, 24)], dice[1:], new_player_obs)\n",
    "\n",
    "                    elif pos > 0 and opponent_obs['board'][(idx + dice[0]) + 23 - 2 * (idx + dice[0])][1] == 0:\n",
    "                        new_player_obs = deepcopy(player_obs)\n",
    "                        new_player_obs['board'][idx] -= 1\n",
    "                        new_player_obs['board'][idx + dice[0]] += 1\n",
    "                        # recursively call function\n",
    "                        find_board_actions(action.copy() + [(idx, idx + dice[0])], dice[1:], new_player_obs)\n",
    "                            \n",
    "            # if we couldn't move and reach return we recursively call the function with the same state and action but iterated dice \n",
    "            find_board_actions(action.copy(), dice[1:], player_obs)\n",
    "    \n",
    "\n",
    "        find_board_actions(action, dice, player_obs)\n",
    "        \n",
    "        # only keep actions if they have the max length\n",
    "        length = max(len(x) for x in legal_actions)\n",
    "        legal_actions = list(l for l in legal_actions if len(l) == length)\n",
    "          \n",
    "        states = [] \n",
    "        # call function that returns the state\n",
    "        for action in legal_actions:\n",
    "            state = env.step(True, action)\n",
    "            states.append(state.copy())\n",
    "\n",
    "        values = []\n",
    "        for state in states:\n",
    "            values.append(self.network.call(state.reshape(1,-1)))\n",
    "\n",
    "        print(values[0][0][0])\n",
    "        \n",
    "        # !!! missing something that evaluates at which value index we have the best probability of winning !!!\n",
    "        # maybe using numpy arrays??\n",
    "        # !!! PLACEHOLDER !!!\n",
    "        action = choice(legal_actions)\n",
    "        \n",
    "        print(\"Action: \", action)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8263263-79b5-4388-b4ab-926cfd303873",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BackgammonEnv()\n",
    "network = Network()\n",
    "agent = Agent(env, network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08d9adf0-f748-47ac-baa0-a5d3ac93c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4751453, shape=(), dtype=float32)\n",
      "Action:  [(16, 21), (16, 21), (16, 21), (11, 16)]\n",
      "tf.Tensor(0.37817866, shape=(), dtype=float32)\n",
      "Action:  [(0, 1), (11, 15)]\n",
      "tf.Tensor(0.41647178, shape=(), dtype=float32)\n",
      "Action:  [(0, 6), (11, 16)]\n",
      "tf.Tensor(0.43557173, shape=(), dtype=float32)\n",
      "Action:  [(18, 22), (1, 6)]\n",
      "tf.Tensor(0.44572604, shape=(), dtype=float32)\n",
      "Action:  [(11, 14), (16, 19), (19, 22), (18, 21)]\n",
      "tf.Tensor(0.506045, shape=(), dtype=float32)\n",
      "Action:  [(16, 20), (11, 13)]\n",
      "tf.Tensor(0.47235626, shape=(), dtype=float32)\n",
      "Action:  [(14, 17), (16, 19), (19, 22), (18, 21)]\n",
      "tf.Tensor(0.4539251, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (13, 18)]\n",
      "tf.Tensor(0.4835693, shape=(), dtype=float32)\n",
      "Action:  [(11, 17), (18, 21)]\n",
      "tf.Tensor(0.5121018, shape=(), dtype=float32)\n",
      "Action:  [(4, 9), (16, 19)]\n",
      "tf.Tensor(0.5973031, shape=(), dtype=float32)\n",
      "Action:  [(17, 22), (11, 13)]\n",
      "tf.Tensor(0.6902096, shape=(), dtype=float32)\n",
      "Action:  [(19, 21), (11, 17)]\n",
      "tf.Tensor(0.5989415, shape=(), dtype=float32)\n",
      "Action:  [(-1, 2), (2, 3)]\n",
      "tf.Tensor(0.56378204, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (-1, 4)]\n",
      "tf.Tensor(0.6655654, shape=(), dtype=float32)\n",
      "Action:  [(0, 2), (22, 24)]\n",
      "tf.Tensor(0.6155484, shape=(), dtype=float32)\n",
      "Action:  [(9, 10), (16, 22)]\n",
      "tf.Tensor(0.6991487, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (18, 22), (4, 8), (22, 24)]\n",
      "tf.Tensor(0.5800189, shape=(), dtype=float32)\n",
      "Action:  [(-1, 6), (4, 5)]\n",
      "tf.Tensor(0.638092, shape=(), dtype=float32)\n",
      "Action:  [(-1, 6), (-1, 6), (3, 9), (8, 14)]\n",
      "tf.Tensor(0.62770647, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (6, 10), (5, 9), (18, 22)]\n",
      "tf.Tensor(0.48318377, shape=(), dtype=float32)\n",
      "Action:  [(-1, 3), (6, 9)]\n",
      "tf.Tensor(0.5765992, shape=(), dtype=float32)\n",
      "Action:  [(4, 9), (9, 11)]\n",
      "tf.Tensor(0.5765298, shape=(), dtype=float32)\n",
      "Action:  [(6, 8), (2, 6)]\n",
      "tf.Tensor(0.5597511, shape=(), dtype=float32)\n",
      "Action:  [(10, 16), (11, 17), (10, 16), (11, 17)]\n",
      "tf.Tensor(0.43579373, shape=(), dtype=float32)\n",
      "Action:  []\n",
      "tf.Tensor(0.46459836, shape=(), dtype=float32)\n",
      "Action:  [(4, 10), (11, 16)]\n",
      "tf.Tensor(0.51833194, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (21, 23)]\n",
      "tf.Tensor(0.49574825, shape=(), dtype=float32)\n",
      "Action:  []\n",
      "tf.Tensor(0.55591756, shape=(), dtype=float32)\n",
      "Action:  [(8, 10), (10, 13)]\n",
      "tf.Tensor(0.47269347, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4)]\n",
      "tf.Tensor(0.47559384, shape=(), dtype=float32)\n",
      "Action:  [(3, 4), (13, 18)]\n",
      "tf.Tensor(0.35554245, shape=(), dtype=float32)\n",
      "Action:  [(-1, 6), (17, 23)]\n",
      "tf.Tensor(0.5373599, shape=(), dtype=float32)\n",
      "Action:  [(9, 15), (15, 21), (9, 15), (15, 21)]\n",
      "tf.Tensor(0.4689491, shape=(), dtype=float32)\n",
      "Action:  [(9, 15), (4, 5)]\n",
      "tf.Tensor(0.6040284, shape=(), dtype=float32)\n",
      "Action:  [(-1, 3), (3, 8)]\n",
      "tf.Tensor(0.59947956, shape=(), dtype=float32)\n",
      "Action:  [(-1, 6), (16, 22)]\n",
      "tf.Tensor(0.65088856, shape=(), dtype=float32)\n",
      "Action:  [(4, 6), (22, 23)]\n",
      "tf.Tensor(0.5708708, shape=(), dtype=float32)\n",
      "Action:  [(-1, 1), (16, 18)]\n",
      "tf.Tensor(0.69013304, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (6, 11)]\n",
      "tf.Tensor(0.60852915, shape=(), dtype=float32)\n",
      "Action:  [(6, 12), (1, 6)]\n",
      "tf.Tensor(0.7679992, shape=(), dtype=float32)\n",
      "Action:  [(-1, 2), (4, 7)]\n",
      "tf.Tensor(0.70727855, shape=(), dtype=float32)\n",
      "Action:  [(-1, 3), (12, 15), (3, 6), (5, 8)]\n",
      "tf.Tensor(0.7610935, shape=(), dtype=float32)\n",
      "Action:  [(-1, 6), (2, 8)]\n",
      "tf.Tensor(0.741512, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (18, 22)]\n",
      "tf.Tensor(0.75154465, shape=(), dtype=float32)\n",
      "Action:  [(21, 23), (21, 24)]\n",
      "tf.Tensor(0.60069877, shape=(), dtype=float32)\n",
      "Action:  [(8, 13), (6, 11), (13, 18), (18, 23)]\n",
      "tf.Tensor(0.73701984, shape=(), dtype=float32)\n",
      "Action:  [(8, 9), (21, 24)]\n",
      "tf.Tensor(0.68749636, shape=(), dtype=float32)\n",
      "Action:  [(6, 7), (4, 9)]\n",
      "tf.Tensor(0.57354194, shape=(), dtype=float32)\n",
      "Action:  [(9, 12), (4, 7), (7, 10), (6, 9)]\n",
      "tf.Tensor(0.5039182, shape=(), dtype=float32)\n",
      "Action:  [(-1, 1), (9, 11)]\n",
      "tf.Tensor(0.64279276, shape=(), dtype=float32)\n",
      "Action:  [(-1, 3), (7, 10)]\n",
      "tf.Tensor(0.5132417, shape=(), dtype=float32)\n",
      "Action:  [(7, 9), (1, 3), (6, 8), (18, 20)]\n",
      "tf.Tensor(0.5317393, shape=(), dtype=float32)\n",
      "Action:  [(-1, 3), (9, 12)]\n",
      "tf.Tensor(0.5204732, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (-1, 4), (18, 22), (8, 12)]\n",
      "tf.Tensor(0.59450614, shape=(), dtype=float32)\n",
      "Action:  [(12, 16), (16, 22)]\n",
      "tf.Tensor(0.6264733, shape=(), dtype=float32)\n",
      "Action:  [(18, 22), (12, 18)]\n",
      "tf.Tensor(0.586361, shape=(), dtype=float32)\n",
      "Action:  [(10, 13), (3, 4)]\n",
      "tf.Tensor(0.63831055, shape=(), dtype=float32)\n",
      "Action:  [(3, 8), (8, 14)]\n",
      "tf.Tensor(0.61082286, shape=(), dtype=float32)\n",
      "Action:  [(22, 23), (21, 24)]\n",
      "tf.Tensor(0.6418211, shape=(), dtype=float32)\n",
      "Action:  [(14, 16), (22, 23)]\n",
      "tf.Tensor(0.578078, shape=(), dtype=float32)\n",
      "Action:  [(23, 24), (4, 7)]\n",
      "tf.Tensor(0.5973461, shape=(), dtype=float32)\n",
      "Action:  [(-1, 5), (9, 14)]\n",
      "tf.Tensor(0.60972077, shape=(), dtype=float32)\n",
      "Action:  [(23, 24), (10, 14)]\n",
      "tf.Tensor(0.5789188, shape=(), dtype=float32)\n",
      "Action:  [(5, 6), (18, 23)]\n",
      "tf.Tensor(0.66281414, shape=(), dtype=float32)\n",
      "Action:  [(7, 10), (14, 15)]\n",
      "tf.Tensor(0.6301074, shape=(), dtype=float32)\n",
      "Action:  [(18, 20), (20, 23)]\n",
      "tf.Tensor(0.6449102, shape=(), dtype=float32)\n",
      "Action:  [(21, 22), (23, 24)]\n",
      "tf.Tensor(0.57286394, shape=(), dtype=float32)\n",
      "Action:  [(6, 9), (14, 15)]\n",
      "tf.Tensor(0.51701236, shape=(), dtype=float32)\n",
      "Action:  [(21, 24), (21, 24)]\n",
      "tf.Tensor(0.5090727, shape=(), dtype=float32)\n",
      "Action:  [(4, 8), (9, 15)]\n",
      "tf.Tensor(0.5605798, shape=(), dtype=float32)\n",
      "Action:  [(-1, 6), (23, 24)]\n",
      "tf.Tensor(0.5498808, shape=(), dtype=float32)\n",
      "Action:  [(15, 20), (23, 24)]\n",
      "tf.Tensor(0.6133292, shape=(), dtype=float32)\n",
      "Action:  [(10, 11), (22, 23), (13, 14), (14, 15)]\n",
      "tf.Tensor(0.46014005, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (4, 8)]\n",
      "tf.Tensor(0.5385826, shape=(), dtype=float32)\n",
      "Action:  [(-1, 5), (5, 6)]\n",
      "tf.Tensor(0.5635644, shape=(), dtype=float32)\n",
      "Action:  [(22, 24), (15, 19), (8, 12), (4, 8)]\n",
      "tf.Tensor(0.5773626, shape=(), dtype=float32)\n",
      "Action:  []\n",
      "tf.Tensor(0.4986857, shape=(), dtype=float32)\n",
      "Action:  [(19, 24), (22, 23)]\n",
      "tf.Tensor(0.54331225, shape=(), dtype=float32)\n",
      "Action:  [(-1, 2), (2, 6)]\n",
      "tf.Tensor(0.52323216, shape=(), dtype=float32)\n",
      "Action:  [(12, 13), (22, 24)]\n",
      "tf.Tensor(0.5116028, shape=(), dtype=float32)\n",
      "Action:  [(6, 12), (6, 9)]\n",
      "tf.Tensor(0.4587389, shape=(), dtype=float32)\n",
      "Action:  [(23, 24), (13, 18)]\n",
      "tf.Tensor(0.5332497, shape=(), dtype=float32)\n",
      "Action:  [(12, 15), (6, 9), (9, 12), (23, 24)]\n",
      "tf.Tensor(0.47072262, shape=(), dtype=float32)\n",
      "Action:  [(-1, 4), (20, 24)]\n",
      "tf.Tensor(0.46116996, shape=(), dtype=float32)\n",
      "Action:  [(12, 13), (15, 21)]\n",
      "tf.Tensor(0.44375026, shape=(), dtype=float32)\n",
      "Action:  [(4, 6), (23, 24)]\n",
      "tf.Tensor(0.5967814, shape=(), dtype=float32)\n",
      "Action:  [(13, 16), (9, 14)]\n",
      "tf.Tensor(0.4376393, shape=(), dtype=float32)\n",
      "Action:  [(23, 24), (6, 10)]\n",
      "tf.Tensor(0.63344336, shape=(), dtype=float32)\n",
      "Action:  [(21, 24), (14, 20)]\n",
      "tf.Tensor(0.41573018, shape=(), dtype=float32)\n",
      "Action:  [(10, 15), (23, 24)]\n",
      "tf.Tensor(0.58881676, shape=(), dtype=float32)\n",
      "Action:  [(20, 21), (16, 19)]\n",
      "tf.Tensor(0.5117635, shape=(), dtype=float32)\n",
      "Action:  [(23, 24), (22, 24)]\n",
      "tf.Tensor(0.5764339, shape=(), dtype=float32)\n",
      "Action:  [(21, 24), (19, 24)]\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "observation = env.reset()\n",
    "while not done:\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done = env.step(False, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "78de4084-868a-427b-a2b1-03b266059026",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = []\n",
    "for a in action:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07446c-4625-4bbc-877e-edf1c3120a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
