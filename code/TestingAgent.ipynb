{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e7ff66-7905-4f12-9330-2189d8dca11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 16:50:42.249351: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-28 16:50:42.253419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/compat/:\n",
      "2022-08-28 16:50:42.253431: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from dice import roll\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.spaces import Dict, Box, Discrete\n",
    "import numpy as np\n",
    "from random import random, choice\n",
    "from copy import copy, deepcopy\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a23a20-5307-42d1-a02d-e86759f41d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgammonEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        # defining some ranges for our observation space\n",
    "        low = np.zeros((96,)) # 4 per each of the 24 positions on the board\n",
    "        high = np.ones((96,)) # the first three of the four values encoding the number of checkers at a given position are either 0 or 1...\n",
    "\n",
    "        for i in range(3, 97, 4): # but every fourth value can go as high as (15-3)/2 = 6...\n",
    "            high[i] = 6.0\n",
    "\n",
    "        # to make them more readable to humans, our observations are dictionaries\n",
    "        # but we can't feed dictionaries to our ANN, so we are going to have to use gym's 'FlattenObservation' wrapper to flatten the dictionary into a single array later\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                # player 'White'\n",
    "                'W': Dict(\n",
    "                    {\n",
    "                        'board': Box(low=low, high=high,dtype=np.float32), # the board, consisting of the bar and 24 'points'\n",
    "                        'barmen': Box(low=0.0, high=7.5,shape=(1,),dtype=np.float32), # and the very first value, which encodes the number of checkers on the bar, can go as high as 15/2 = 7.5\n",
    "                        'menoff': Box(low=0.0,high=1.0,shape=(1,),dtype=np.float32), # number of checkers removed from the board as a fraction of the total number of checkers i.e. n/15\n",
    "                        'turn': Discrete(2) # '1' if it is this player's turn, '0' if not\n",
    "                    }\n",
    "                ),\n",
    "                # player 'Black'\n",
    "                'B': Dict(\n",
    "                    {\n",
    "                        'board': Box(low=low, high=high,dtype=np.float32),\n",
    "                        'barmen': Box(low=0.0,high=7.5,shape=(1,),dtype=np.float32), \n",
    "                        'menoff': Box(low=0.0,high=1.0,shape=(1,),dtype=np.float32),\n",
    "                        'turn': Discrete(2)\n",
    "                    }\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # which actions can be taken at any given time step depend on the state of the board and the roll of the dice. hence we choose to not attempt to define the action space\n",
    "        self.action_space = None\n",
    "        \n",
    "        # create an empty board:\n",
    "        # to allow for indexing of board positions, the environment's state uses an array of arrays to store the values of the bar and each point\n",
    "        # later we simply flatten this array to create observations\n",
    "\n",
    "        self.state = {\n",
    "            'W': {\n",
    "                'board': np.zeros((24, 4)),\n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': 0\n",
    "            },\n",
    "            'B': {\n",
    "                'board': np.zeros((24, 4)),\n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # the previously mentioned truncated unary encoding:\n",
    "        self.encoding = {\n",
    "            0: np.array([0.,0.,0.,0.]),\n",
    "            1: np.array([1.,0.,0.,0.]),\n",
    "            2: np.array([1.,1.,0.,0.]),\n",
    "            3: np.array([1.,1.,1.,0.]),\n",
    "            4: np.array([1.,1.,1.,0.5]),\n",
    "            5: np.array([1.,1.,1.,1.]),\n",
    "            6: np.array([1.,1.,1.,1.5]),\n",
    "            7: np.array([1.,1.,1.,2.0]),\n",
    "            8: np.array([1.,1.,1.,2.5]),\n",
    "            9: np.array([1.,1.,1.,3.0]),\n",
    "            10: np.array([1.,1.,1.,3.5]),\n",
    "            11: np.array([1.,1.,1.,4.0]),\n",
    "            12: np.array([1.,1.,1.,4.5]),\n",
    "            13: np.array([1.,1.,1.,5.0]),\n",
    "            14: np.array([1.,1.,1.,5.5]),\n",
    "            15: np.array([1.,1.,1.,6.0])\n",
    "        }\n",
    "\n",
    "        # define the game's starting position:\n",
    "  \n",
    "        # create an empty board\n",
    "        self.starting_pos = np.zeros((24, 4))\n",
    "\n",
    "        # place the correct number of checkers in the correct positions\n",
    "        self.starting_pos[0] = copy(self.encoding[2])\n",
    "        self.starting_pos[11] = copy(self.encoding[5])\n",
    "        self.starting_pos[16] = copy(self.encoding[3])\n",
    "        self.starting_pos[18] = copy(self.encoding[5])\n",
    "\n",
    "    def _flatten_obs(self, obs):\n",
    "        w_board = obs['W']['board']\n",
    "        w_board = w_board.flatten()\n",
    "        b_board = obs['B']['board']\n",
    "        b_board = b_board.flatten()\n",
    "\n",
    "        observation = []\n",
    "        observation = np.append(observation,w_board)\n",
    "        observation = np.append(observation,obs['W']['barmen'])\n",
    "        observation = np.append(observation,obs['W']['menoff'])\n",
    "        observation = np.append(observation,obs['W']['turn'])\n",
    "        observation = np.append(observation,b_board)\n",
    "        observation = np.append(observation,obs['B']['barmen'])\n",
    "        observation = np.append(observation,obs['B']['menoff'])\n",
    "        observation = np.append(observation,obs['B']['turn'])\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_info(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self): \n",
    "\n",
    "        # a 'coin flip' to determine which side goes first\n",
    "        coin = int(random()>0.5)\n",
    "\n",
    "        # reset the board to the game's starting position and assign a turn order based on the coin flip\n",
    "        self.state = {\n",
    "            'W': {\n",
    "                'board':copy(self.starting_pos), \n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': coin\n",
    "            },\n",
    "            'B': {\n",
    "                'board':copy(self.starting_pos),\n",
    "                'barmen': 0,\n",
    "                'menoff': 0,\n",
    "                'turn': 1-coin\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return copy(self.state)\n",
    "\n",
    "    def step(self, persistent, action):\n",
    "\n",
    "        if persistent:\n",
    "            state = self.state\n",
    "        else:\n",
    "            state = deepcopy(self.state)\n",
    "\n",
    "        # who's turn is it?\n",
    "        if self.state['W']['turn'] == 1:\n",
    "            player = 'W'\n",
    "            opponent = 'B'\n",
    "        else:\n",
    "            player = 'B'\n",
    "            opponent = 'W'\n",
    "\n",
    "        for move in action:\n",
    "            # 'LIFTING' A CHECKER\n",
    "\n",
    "            old_pos, new_pos = move\n",
    "            \n",
    "            # are we moving a piece off the bar?\n",
    "            if old_pos == -1:\n",
    "                # remove a checker from the bar\n",
    "                state[player]['barmen'] -= 0.5\n",
    "\n",
    "            else:\n",
    "                # get the current number of checkers at the position from which we need to remove a checker\n",
    "                encoded_checkers = state[player]['board'][old_pos]\n",
    "                # decode\n",
    "                for key, value in self.encoding.items():\n",
    "                    if np.array_equal(encoded_checkers,value):\n",
    "                        n_checkers = key\n",
    "                # subtract a checker\n",
    "                state[player]['board'][old_pos] = copy(self.encoding[n_checkers-1])\n",
    "\n",
    "            # 'PLACING DOWN' A CHECKER\n",
    "\n",
    "            # are we bearing off?\n",
    "            if new_pos == 24:\n",
    "                state[player]['menoff'] += 1/15\n",
    "\n",
    "            else:\n",
    "                # get the current number of checkers at the position to which we need to add a checker\n",
    "                encoded_checkers = state[player]['board'][new_pos]\n",
    "                # decode\n",
    "                for key, value in self.encoding.items():\n",
    "                    if np.array_equal(encoded_checkers,value):\n",
    "                        n_checkers = key\n",
    "                # add a checker\n",
    "                state[player]['board'][new_pos] = copy(self.encoding[n_checkers+1])\n",
    "\n",
    "                # check for blots\n",
    "                mirror_pos = new_pos+23-2*new_pos\n",
    "                if not np.array_equal(state[opponent]['board'][mirror_pos],[0,0,0,0]):\n",
    "                    # if there is a blot, move the opponent's piece to the bar\n",
    "                    state[opponent]['board'][mirror_pos] = [0,0,0,0]\n",
    "                    state[opponent]['barmen'] += 0.5\n",
    "\n",
    "        # update the turn order\n",
    "        state['W']['turn'] = 1 - state['W']['turn']\n",
    "        state['B']['turn'] = 1 - state['B']['turn']\n",
    "\n",
    "        # if this is only a 'simulated' step, return the new state here\n",
    "        if not persistent:\n",
    "            return self._flatten_obs(state)\n",
    "\n",
    "        # reward is zero unless one of four conditions is met:\n",
    "        reward = 0\n",
    "\n",
    "        # 1) White wins, Black is gammoned\n",
    "        if self.state['W']['menoff'] > 0.9 and self.state['B']['menoff'] == 0:\n",
    "            reward = 2\n",
    "        # 2) White wins\n",
    "        elif self.state['W']['menoff'] > 0.9:\n",
    "            reward = 1\n",
    "        # 3) Black wins, White is gammoned\n",
    "        elif self.state['B']['menoff'] > 0.9 and self.state['W']['menoff'] == 0: \n",
    "            reward = -2\n",
    "        # 4) Black wins\n",
    "        elif self.state['B']['menoff'] > 0.9:\n",
    "            reward = -1\n",
    "\n",
    "        # if one of the four conditions is met, the game is finished and the episode ends\n",
    "        done = reward != 0\n",
    "\n",
    "        return copy(self.state), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2747007-0ebe-459b-a187-6cd90119a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, gamma, lam, network):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.network = network\n",
    "        self.eligibility_trace = np.zeros_like(self.network.layers[0].get_weights()[0])\n",
    "        self.state = np.zeros_like(self.env._flatten_obs(env.observation_space))\n",
    "        self.state_ = np.zeros_like(self.env._flatten_obs(env.observation_space))\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        \n",
    "        dice = roll()\n",
    "        \n",
    "        legal_actions = []\n",
    "        size = len(dice)\n",
    "        action = []\n",
    "\n",
    "        # check whose turn it is\n",
    "        if obs['W']['turn'] == 1:\n",
    "            player = 'W'\n",
    "            opponent = 'B'\n",
    "        else:\n",
    "            player = 'B'\n",
    "            opponent = 'W'\n",
    "            \n",
    "        player_obs = deepcopy(obs[player])\n",
    "        opponent_obs = obs[opponent]\n",
    "        \n",
    "        # make board more easily readable\n",
    "        new_board = []\n",
    "        for pos in player_obs['board']:\n",
    "            new_board.append(pos[0] + pos[1] + pos[2] + pos[3] * 2)\n",
    "        player_obs['board'] = new_board\n",
    "        \n",
    "        # !!! just for testing purposes !!!\n",
    "        # if not length == size and not len(action) == 0:\n",
    "        #test_opponent = deepcopy(opponent_obs)\n",
    "        #new_op_board = []\n",
    "        #for idx, pos in enumerate(test_opponent['board']):\n",
    "        #    new_op_board.append(pos[0] + pos[1] + pos[2] + pos[3] * 2)\n",
    "        #test_opponent['board'] = new_op_board\n",
    "        #print(\"Dice: \", dice)\n",
    "        #print(\"Player:\\t\\t\", player, player_obs['board'])\n",
    "        #print(\"Opponent:\\t\", opponent, test_opponent['board'])\n",
    "        \n",
    "        # recursive function to search \"action-tree\"\n",
    "        def find_board_actions(action, dice, player_obs):\n",
    "            \n",
    "            # if we have iterated through all dice the action is appended and we return\n",
    "            if len(dice) == 0:\n",
    "                legal_actions.append(action)\n",
    "                return\n",
    "\n",
    "            # in case there are chips in the bar they have to be removed before any other actions can be taken\n",
    "            if player_obs['barmen'] > 0:\n",
    "                # check for free points/blots\n",
    "                for i, die in enumerate(dice):\n",
    "                    # for each die, check if the barmen would land on a free spot/a blot\n",
    "                    if opponent_obs['board'][die + 23 - 2 * die][1] == 0:\n",
    "                        # create a new observation in which the barman has been freed\n",
    "                        new_player_obs = deepcopy(player_obs)\n",
    "                        new_player_obs['board'][die] += 1\n",
    "                        new_player_obs['barmen'] -= 0.5\n",
    "                        # remove the used die\n",
    "                        new_dice = copy(dice)\n",
    "                        new_dice.pop(i)\n",
    "                        # append the chosen move to action, pass new observation and dice to the recursion\n",
    "                        find_board_actions(action.copy() + [(-1, die)], new_dice, new_player_obs)\n",
    "                        \n",
    "            else:\n",
    "                # is it legal to move off the board?\n",
    "                bearingoff = True\n",
    "                # check for checkers in the first three quadrants\n",
    "                # NOTE: easier to just add up all the values and compare them to zero?\n",
    "                for idx in range(18):\n",
    "                    if player_obs['board'][idx] > 0:\n",
    "                        bearingoff = False\n",
    "                        break\n",
    "                        \n",
    "                # iterate through all positions and check if we can move to position + current dice\n",
    "                for pos, n_checkers in enumerate(player_obs['board']):\n",
    "                    # check if a dice roll could exceed the limitations of the board\n",
    "                    if n_checkers > 0 and (pos + dice[0]) > 23:\n",
    "                        # can a checker be moved off the board\n",
    "                        if bearingoff:\n",
    "                            if pos+dice[0] == 24 or sum(player_obs['board'][:pos]) == 0:\n",
    "                                # create a new observation\n",
    "                                new_player_obs = deepcopy(player_obs)\n",
    "                                new_player_obs['board'][pos] -= 1\n",
    "                                # recursively call function\n",
    "                                find_board_actions(action.copy() + [(pos, 24)], dice[1:], new_player_obs)       \n",
    "                            \n",
    "                    # If there is no checker on the point indicated by the roll, the player must make a legal move using a checker on a higher-numbered point. If there are no checkers on higher-numbered points, the player is permitted (and required) to remove a checker from the highest point on which one of his checkers resides.\n",
    "                    \n",
    "                    # move a checker only if the move is legal\n",
    "                    elif n_checkers > 0 and opponent_obs['board'][(pos + dice[0]) + 23 - 2 * (pos + dice[0])][1] == 0:\n",
    "                        new_player_obs = deepcopy(player_obs)\n",
    "                        new_player_obs['board'][pos] -= 1\n",
    "                        new_player_obs['board'][pos + dice[0]] += 1\n",
    "                        # recursively call function\n",
    "                        find_board_actions(action.copy() + [(pos, pos + dice[0])], dice[1:], new_player_obs)\n",
    "                            \n",
    "            # if we couldn't move and reach return we recursively call the function with the same state and action but iterated dice \n",
    "            find_board_actions(action.copy(), dice[1:], player_obs)\n",
    "    \n",
    "        # call the recursive function\n",
    "        find_board_actions(action, dice, player_obs)\n",
    "        \n",
    "        # only keep actions if they have the max length\n",
    "        length = max(len(x) for x in legal_actions)\n",
    "        legal_actions = list(l for l in legal_actions if len(l) == length)\n",
    "          \n",
    "        states = [] \n",
    "        # call function that returns the state\n",
    "        for action in legal_actions:\n",
    "            state = env.step(False, action)\n",
    "            states.append(state.copy())\n",
    "\n",
    "        values = []\n",
    "        for state in states:\n",
    "            value = self.network.call(state.reshape(1,-1))[0]\n",
    "            if player == 'W':\n",
    "                values.append(float(value[0] + 2 * value[1] - value[2] - 2 * value[3]))\n",
    "            else:\n",
    "                values.append(float(-value[0] - 2 * value[1] + value[2] + 2 * value[3]))\n",
    "           \n",
    "        index = values.index(max(values))\n",
    "        action = legal_actions[index]\n",
    "        self.state_ = states[index]\n",
    "        \n",
    "        print(\"Action: \", action)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # update the eligibility trace\n",
    "        self.eligibility_trace = self.gamma * self.lam * self.eligibility_trace + TD error???\n",
    "        \n",
    "        # update the weights\n",
    "        weights = self.network.layers[0].get_weights[0] + self.alpha * (reward + self.gamma * TD-Error) * self.eligibility_trace\n",
    "        self.network.layers[0].set_weights(weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8263263-79b5-4388-b4ab-926cfd303873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marie/anaconda3/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "2022-08-16 12:31:20.802380: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-16 12:31:20.802411: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-16 12:31:20.802436: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (maries-debian): /proc/driver/nvidia/version does not exist\n",
      "2022-08-16 12:31:20.802938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "env = BackgammonEnv()\n",
    "network = Network()\n",
    "agent = Agent(env, network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d9adf0-f748-47ac-baa0-a5d3ac93c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  [(18, 19), (11, 16)]\n",
      "Action:  [(16, 22), (18, 21)]\n",
      "Action:  [(0, 2), (18, 21)]\n",
      "Action:  [(-1, 3), (16, 19), (16, 19), (18, 21)]\n",
      "Action:  [(-1, 2), (16, 21)]\n",
      "Action:  [(-1, 3), (18, 20)]\n",
      "Action:  [(11, 17), (2, 3)]\n",
      "Action:  [(-1, 3), (3, 9)]\n",
      "Action:  [(18, 21), (11, 13)]\n",
      "Action:  [(3, 9), (18, 20)]\n",
      "Action:  [(-1, 5), (11, 13)]\n",
      "Action:  []\n",
      "Action:  [(13, 15), (13, 15), (15, 17), (15, 17)]\n",
      "Action:  [(-1, 4), (11, 16)]\n",
      "Action:  [(-1, 5), (18, 19)]\n",
      "Action:  []\n",
      "Action:  [(11, 17), (17, 18)]\n",
      "Action:  [(-1, 1), (16, 21)]\n",
      "Action:  [(18, 20), (18, 22)]\n",
      "Action:  [(-1, 4), (-1, 4), (19, 23), (19, 23)]\n",
      "Action:  [(-1, 2), (-1, 4)]\n",
      "Action:  [(-1, 3), (9, 10)]\n",
      "Action:  [(-1, 3), (4, 8)]\n",
      "Action:  [(-1, 1), (11, 17)]\n",
      "Action:  [(-1, 5), (8, 9)]\n",
      "Action:  [(17, 21), (4, 5)]\n",
      "Action:  [(-1, 5), (5, 6)]\n",
      "Action:  [(11, 17), (17, 20)]\n",
      "Action:  [(-1, 2), (-1, 5)]\n",
      "Action:  []\n",
      "Action:  [(5, 10), (10, 13)]\n",
      "Action:  [(-1, 5)]\n",
      "Action:  [(9, 14), (14, 20)]\n",
      "Action:  [(-1, 5), (-1, 4)]\n",
      "Action:  [(2, 3), (13, 16)]\n",
      "Action:  [(-1, 1)]\n",
      "Action:  [(5, 9), (9, 11)]\n",
      "Action:  [(-1, 4), (4, 10)]\n",
      "Action:  [(11, 14), (14, 20)]\n",
      "Action:  [(1, 5), (11, 17)]\n",
      "Action:  [(5, 10), (10, 16)]\n",
      "Action:  [(11, 14), (14, 17), (17, 20), (17, 20)]\n",
      "Action:  [(-1, 2), (2, 4), (4, 6), (20, 22)]\n",
      "Action:  [(-1, 3), (5, 9)]\n",
      "Action:  [(-1, 2), (6, 12)]\n",
      "Action:  [(0, 4), (0, 1)]\n",
      "Action:  [(-1, 2), (16, 22)]\n",
      "Action:  [(-1, 1), (3, 9)]\n",
      "Action:  [(-1, 1), (2, 5)]\n",
      "Action:  [(-1, 3), (5, 8), (10, 13), (13, 16)]\n",
      "Action:  [(16, 20), (20, 22)]\n",
      "Action:  [(-1, 1)]\n",
      "Action:  [(-1, 6), (6, 9)]\n",
      "Action:  [(-1, 4), (8, 9)]\n",
      "Action:  [(1, 7), (9, 12)]\n",
      "Action:  [(-1, 5), (20, 21)]\n",
      "Action:  [(-1, 1), (7, 11)]\n",
      "Action:  [(1, 3), (5, 10)]\n",
      "Action:  [(1, 2), (2, 5)]\n",
      "Action:  []\n",
      "Action:  [(5, 10), (10, 15), (11, 16), (15, 20)]\n",
      "Action:  [(-1, 3), (-1, 4)]\n",
      "Action:  [(-1, 2), (5, 11)]\n",
      "Action:  [(3, 5), (20, 21)]\n",
      "Action:  [(-1, 1), (1, 6)]\n",
      "Action:  [(10, 12), (12, 15)]\n",
      "Action:  [(-1, 2), (5, 7), (7, 9), (9, 11)]\n",
      "Action:  [(-1, 4), (15, 20)]\n",
      "Action:  [(6, 9), (2, 3)]\n",
      "Action:  [(-1, 4)]\n",
      "Action:  [(9, 15), (15, 17)]\n",
      "Action:  [(5, 10), (5, 9)]\n",
      "Action:  [(17, 20), (11, 16)]\n",
      "Action:  [(9, 12), (12, 15), (15, 18), (18, 21)]\n",
      "Action:  [(20, 23), (16, 20)]\n",
      "Action:  [(10, 12), (12, 15)]\n",
      "Action:  [(3, 5), (20, 23)]\n",
      "Action:  [(5, 10), (15, 17)]\n",
      "Action:  [(16, 18), (18, 21)]\n",
      "Action:  [(10, 15), (15, 17)]\n",
      "Action:  [(5, 11), (17, 23), (17, 23), (17, 23)]\n",
      "Action:  [(4, 8), (8, 10)]\n",
      "Action:  [(11, 15), (15, 20)]\n",
      "Action:  [(4, 8), (8, 13)]\n",
      "Action:  [(21, 23), (20, 23)]\n",
      "Action:  [(13, 19), (19, 20)]\n",
      "Action:  [(16, 22), (22, 23)]\n",
      "Action:  [(4, 10), (10, 15)]\n",
      "Action:  [(12, 13), (13, 15)]\n",
      "Action:  [(-1, 3), (15, 18), (17, 20), (18, 21)]\n",
      "Action:  [(21, 23), (15, 16)]\n",
      "Action:  [(4, 8), (17, 22)]\n",
      "Action:  [(12, 16), (16, 18)]\n",
      "Action:  [(3, 4), (8, 9), (9, 10), (20, 21)]\n",
      "Action:  [(18, 21), (21, 23)]\n",
      "Action:  [(10, 13), (4, 10)]\n",
      "Action:  [(16, 22), (16, 21)]\n",
      "Action:  [(10, 16), (13, 18)]\n",
      "Action:  [(16, 18)]\n",
      "Action:  [(4, 9), (9, 10)]\n",
      "Action:  [(23, 24), (18, 23)]\n",
      "Action:  [(4, 5), (16, 17), (18, 19), (19, 20)]\n",
      "Action:  [(21, 24), (23, 24)]\n",
      "Action:  [(10, 16), (17, 18)]\n",
      "Action:  [(21, 24), (21, 24)]\n",
      "Action:  [(4, 8), (8, 11)]\n",
      "Action:  [(23, 24), (22, 24)]\n",
      "Action:  [(16, 18), (11, 17)]\n",
      "Action:  [(23, 24), (23, 24)]\n",
      "Action:  [(17, 23), (22, 23)]\n",
      "Action:  [(23, 24), (23, 24)]\n",
      "Action:  [(18, 23), (21, 23)]\n",
      "Action:  [(23, 24), (23, 24)]\n",
      "Action:  [(20, 21), (21, 23)]\n",
      "Action:  [(23, 24), (23, 24)]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "observation = env.reset()\n",
    "while not done:\n",
    "    action = agent.choose_action(observation)\n",
    "    observation_, reward, done = env.step(True, action)\n",
    "    agent.learn\n",
    "    \n",
    "print(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af07446c-4625-4bbc-877e-edf1c3120a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.zeros(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384f88f-959a-40f7-b03e-cd4dae389d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
